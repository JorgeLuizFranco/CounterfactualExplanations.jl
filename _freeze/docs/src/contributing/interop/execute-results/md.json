{
  "hash": "6775f130e2f85a04954c94d3b90cee7b",
  "result": {
    "markdown": "---\nformat:\n  commonmark:\n    variant: '-raw_html'\n    wrap: none\n    self-contained: true\ncrossref:\n  fig-prefix: Figure\n  tbl-prefix: Table\nbibliography: 'https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib'\noutput: asis\nexecute:\n  echo: true\n  eval: false\n---\n\n```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n# Interoperability\n\n```\n!!! info \"Contributor's Guide\" \n    Our work on language interoperability is still experimental. In this tutorial we demonstrate our early attempts to extend the package to accommodate `torch` models trained in R and Python. The goal here is to provide a template and/or starting point for contributors that would like to add support for other models trained in foreign programming languages. If that contributor could be you, please see the related [issue](https://github.com/pat-alt/CounterfactualExplanations.jl/issues/67) and work in the linked [branch](https://github.com/pat-alt/CounterfactualExplanations.jl/tree/67-interoperability-broken-after-moving-to-julia-18).\n```\n\n```\n!!! info \"Native support currently broken and removed\" \n    After bumping Julia version compatibility to `1.8`, interoperability broke for some reason (it was admittedly always experimental and hacky). The code that was previously used for interoperability was removed from the main branch, but still lives [here](https://github.com/pat-alt/CounterfactualExplanations.jl/tree/67-interoperability-broken-after-moving-to-julia-18).\n```\n\nThe Julia language offers unique support for programming language interoperability. For example, calling Python and R is made remarkably easy through `PyCall.jl` (potentially `PythonCall.jl` is a better option) and `RCall.jl`. In this tutorial we will see how `CounterfactualExplanations.jl` can leverage this functionality. \n\n\n\nTo get started we will first load some two-dimensional toy data:\n\n``` {.julia .cell-code}\nusing Random\n# Some random data:\nRandom.seed!(1234);\nN = 100\nusing CounterfactualExplanations\nusing CounterfactualExplanations.Data\nxs, ys = Data.toy_data_non_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n```\n\n\n## `torch` model trained in R\n\nThe code below builds a simple MLP in R:\n\n``` {.julia .cell-code}\nusing RCall\nR\"\"\"\n# Data\nlibrary(torch)\nX <- torch_tensor(t($X))\nys <- torch_tensor($ys)\n\n# Model:\nmlp <- nn_module(\n  initialize = function() {\n    self$layer1 <- nn_linear(2, 32)\n    self$layer2 <- nn_linear(32, 1)\n  },\n  forward = function(input) {\n    input <- self$layer1(input)\n    input <- nnf_sigmoid(input)\n    input <- self$layer2(input)\n    input\n  }\n)\nmodel <- mlp()\noptimizer <- optim_adam(model$parameters, lr = 0.1)\nloss_fun <- nnf_binary_cross_entropy_with_logits\n\"\"\"\n```\n\n\nThe following code trains the MLP for the binary prediction task at hand:\n\n``` {.julia .cell-code}\nR\"\"\"\nfor (epoch in 1:100) {\n\n  model$train()  \n\n  # Compute prediction and loss:\n  output <- model(X)[,1]\n  loss <- loss_fun(output, ys)\n\n  # Backpropagation:\n  optimizer$zero_grad()\n  loss$backward()\n  optimizer$step()\n  \n  cat(sprintf(\"Loss at epoch %d: %7f\\n\", epoch, loss$item()))\n}\n\"\"\"\n```\n\n\n### Making the model compatible\n\nAs always we need to extend the `logits` and `probs` functions to make the model compatible with `CounterfactualExplanations.jl`. As evident from the code below, this is actually quite straight-forward: the logits are returned by the `torch` model and copied form R into the Julia environment. Probabilities are then computed in Julia, by passing the logits through the sigmoid function.\n\n``` {.julia .cell-code}\nusing Flux\nusing CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct MyRTorchModel <: Models.AbstractDifferentiableModel\n    model::Any\nend\n\n# Step 2)\nfunction logits(M::MyRTorchModel, X::AbstractArray)\n  nn = M.model\n  ŷ = rcopy(R\"as_array($nn(torch_tensor(t($X))))\")\n  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n  return ŷ'\nend\nprobs(M::MyRTorchModel, X::AbstractArray)= σ.(logits(M, X))\nM = MyRTorchModel(R\"model\")\n```\n\n\n### Adapting the generator\n\nNext we need to do a tiny bit of work on the `AbstractGenerator` side. By default methods underlying the `GenericGenerator` are desiged to work with models that have gradient access through `Zygote.jl`, one of Julia's main autodifferentiation packages. Of course, `Zygote.jl` cannot access the gradients of our `torch` model, so we need to adapt the code slightly. Fortunately, it turns out that all we need to do is extend the function that computes the gradient with respect to the loss function for the generic counterfactual search: `∂ℓ(generator::GenericGenerator, x′, M, t)`. In particular, we will extend the function by a method that is specific to the `MyRTorchModel` type we defined above. The code below implements this: our new method `∂ℓ` calls R in order to use `torch`'s autodifferentiation functionality for computing the gradient. \n\n``` {.julia .cell-code}\nimport CounterfactualExplanations.Generators: ∂ℓ\nusing LinearAlgebra\n\n# Counterfactual loss:\nfunction ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyRTorchModel, counterfactual_state::CounterfactualState) \n  nn = M.model\n  x_cf = counterfactual_state.x′\n  t = counterfactual_state.target_encoded\n  R\"\"\"\n  x <- torch_tensor($x_cf, requires_grad=TRUE)\n  output <- $nn(x)\n  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)\n  obj_loss$backward()\n  \"\"\"\n  grad = rcopy(R\"as_array(x$grad)\")\n  return grad\nend\n```\n\n\n### Generating counterfactuals\n\nFrom here on onwards we use the `CounterfactualExplanations.jl` functionality as always. Below we choose a random sample, define our generic generator and finally run the search:\n\n``` {.julia .cell-code}\n# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:length(xs))) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n```\n\n\n``` {.julia .cell-code}\n# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)\n```\n\n\n\n\n![](www/interop_r.gif)\n\n## `torch` model in Python \n\n```\n!!! warning \"'PyTorch' and 'torch for R' interplay\" \n    We have noted that using both ['PyTorch'](https://pytorch.org/) through `PyCall.jl` and ['torch for R'](https://torch.mlverse.org/packages) through `RCall.jl` in the same Julia session causes issues. In particular, loading 'PyTorch' after loading 'torch for R' cause the Julia session to crash and vice versa. For the time being, we therefore advise not to use both `RTorchModel()` and `PyTorchModel` in the same session.\n```\n\nThe steps involved are largely analogous to the above, so we leave the following code uncommented.\n\n``` {.julia .cell-code}\nusing PyCall\npy\"\"\"\n# Data\nimport torch\nfrom torch import nn\nX = torch.Tensor($X).T\nys = torch.Tensor($ys)\n\nclass MLP(nn.Module):\n  def __init__(self):\n    super(MLP, self).__init__()\n    self.model = nn.Sequential(\n      nn.Flatten(),\n      nn.Linear(2, 32),\n      nn.Sigmoid(),\n      nn.Linear(32, 1)\n    )\n\n  def forward(self, x):\n    logits = self.model(x)\n    return logits\n\nmodel = MLP()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fun = nn.BCEWithLogitsLoss()\n\"\"\"\n```\n\n\n``` {.julia .cell-code}\npy\"\"\"\nfor epoch in range(100):\n  # Compute prediction and loss:\n  output = model(X).squeeze()\n  loss = loss_fun(output, ys)\n  \n  # Backpropagation:\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n  print(f\"Loss at epoch {epoch+1}: {loss.item():>7f}\")\n\"\"\"\n```\n\n\n``` {.julia .cell-code}\nusing Flux\nusing CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct MyPyTorchModel <: Models.AbstractDifferentiableModel\n    model::Any\nend\n\n# Step 2)\nfunction logits(M::MyPyTorchModel, X::AbstractArray)\n  nn = M.model\n  if !isa(X, Matrix)\n    X = reshape(X, length(X), 1)\n  end\n  ŷ = py\"$nn(torch.Tensor($X).T).detach().numpy()\"\n  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n  return ŷ'\nend\nprobs(M::MyPyTorchModel, X::AbstractArray)= σ.(logits(M, X))\nM = MyPyTorchModel(py\"model\")\n```\n\n\n``` {.julia .cell-code}\nimport CounterfactualExplanations.Generators: ∂ℓ\nusing LinearAlgebra\n\n# Countefactual loss:\nfunction ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyPyTorchModel, counterfactual_state::CounterfactualState) \n  nn = M.model\n  x′ = counterfactual_state.x′\n  t = counterfactual_state.target_encoded\n  x = reshape(x′, 1, length(x′))\n  py\"\"\"\n  x = torch.Tensor($x)\n  x.requires_grad = True\n  t = torch.Tensor($[t]).squeeze()\n  output = $nn(x).squeeze()\n  obj_loss = nn.BCEWithLogitsLoss()(output,t)\n  obj_loss.backward()\n  \"\"\"\n  grad = vec(py\"x.grad.detach().numpy()\")\n  return grad\nend\n```\n\n\n``` {.julia .cell-code}\n# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:length(xs))) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n```\n\n\n``` {.julia .cell-code}\n# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)\n```\n\n\n\n\n![](www/interop_py.gif)\n\n",
    "supporting": [
      "interop_files"
    ],
    "filters": []
  }
}