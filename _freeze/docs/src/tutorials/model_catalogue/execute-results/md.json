{
  "hash": "74ecc46123c3ee7463bd3f47b1e8cc87",
  "result": {
    "markdown": "```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# Model Catalogue\n\nWhile in general it is assumed that users will use this package to explain their pre-trained models, we provide out-of-the-box functionality to train various simple default models. In this tutorial, we will see how these models can be fitted to `CounterfactualData`.\n\n## Available Models\n\nThe `model_catalogue` can be used to inspect the available default models: \n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nmodel_catalogue\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nDict{Symbol, Any} with 3 entries:\n  :Linear       => Linear\n  :DeepEnsemble => FluxEnsemble\n  :MLP          => FluxModel\n```\n:::\n:::\n\n\nAs with the [`data_catalogue`](@ref), the dictionary keys correspond to the model names. In this case, the dictionary values are constructors that can be used called on instances of type `CounterfactualData` to fit the corresponding model. In most cases, users will find it most convenient to use the [`fit_model`](@ref) API call instead.\n\n## Fitting Models\n\nFirst, let's load one of the synthetic datasets:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nn = 500\ncounterfactual_data = load_multi_class(n)\n```\n:::\n\n\nWe could use a Deep Ensemble [@lakshminarayanan2016simple] as follows:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nM = fit_model(counterfactual_data, :DeepEnsemble)\n```\n:::\n\n\nThe returned object is an instance of type `FluxEnsemble <: AbstractFittedModel` and can be used in downstream tasks without further ado. For example, the resulting fit can be visualised using the generic `plot()` method as:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nplts = []\nfor target in counterfactual_data.y_levels\n    plt = plot(M, counterfactual_data; target=target, title=\"p(y=$(target)|x,Î¸)\")\n    plts = [plts..., plt]\nend\nplot(plts...)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n![](model_catalogue_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\n## Tuning Models\n\nBy default, model architectures are very simple. Through optional arguments, users have some control over the neural network architecture and can choose to impose regularization through dropout. Let's tackle a more challenging dataset: MNIST [@lecun1998mnist]. \n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\ncounterfactual_data = load_mnist(10000)\ntrain_data, test_data = \n    CounterfactualExplanations.DataPreprocessing.train_test_split(counterfactual_data)\n```\n:::\n\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=22}\n![](model_catalogue_files/figure-commonmark/cell-8-output-1.svg){}\n:::\n:::\n\n\nIn this case, we will use a Multi-Layer Perceptron (MLP) but we will adjust the model and training hyperparameters. Parameters related to training of `Flux.jl` models are currently stored in a mutable container:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nflux_training_params\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nCounterfactualExplanations.FluxModelParams\n  loss: Symbol logitbinarycrossentropy\n  opt: Symbol Adam\n  n_epochs: Int64 100\n  batchsize: Int64 1\n  verbose: Bool false\n```\n:::\n:::\n\n\nIn cases like this one, where model training can be expected to take a few moments, it can be useful to activate verbosity, so let's set the corresponding field value to `true`. We'll also impose mini-batch training:\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nflux_training_params.verbose = true\nflux_training_params.batchsize = round(size(train_data.X,2)/10)\n```\n:::\n\n\nTo account for the fact that this is a slightly more challenging task, we will use an appropriate number of hidden neurons per layer. We will also activate dropout regularization. To scale networks up further, it is also possible to adjust the number of hidden layers, which we will not do here.\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nmodel_params = (\n    n_hidden = 32,\n    dropout = true\n)\n```\n:::\n\n\nThe `model_params` can be supplied to the familiar API call:\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nM = fit_model(train_data, :MLP; model_params...)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nFluxModel(Chain(Dense(784 => 32, relu), Dropout(0.25), Dense(32 => 10)), :classification_multi)\n```\n:::\n:::\n\n\nThe model performance on our test set can be evaluated as follows:\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nmodel_evaluation(M, test_data)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n0.9269093812317137\n```\n:::\n:::\n\n\nFinally, let's restore the default training parameters:\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nCounterfactualExplanations.reset!(flux_training_params)\n```\n:::\n\n\n## References\n\n",
    "supporting": [
      "model_catalogue_files"
    ],
    "filters": []
  }
}