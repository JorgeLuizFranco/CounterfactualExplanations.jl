{
  "hash": "31e94685a2e03e9159f3ade8824f5d78",
  "result": {
    "markdown": "---\ntitle: Handling Models\n---\n\n\n```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n\nThe typical use-case for Counterfactual Explanations and Algorithmic Recourse is as follows: users have trained some supervised model that is not inherently interpretable and are looking for a way to explain it. In this tutorial, we will see how pre-trained models can be used with this package.\n\n## Models trained in `Flux.jl`\n\nWe will train a simple binary classifier in `Flux.jl` on the popular Moons dataset:\n\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nn = 500\ncounterfactual_data = load_moons(n)\nX = counterfactual_data.X\ny = counterfactual_data.y\nplt = plot()\nscatter!(counterfactual_data)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](models_files/figure-commonmark/cell-3-output-1.svg){}\n:::\n:::\n\n\nThe following code chunk sets up a Deep Neural Network for the task at hand:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ndata = Flux.DataLoader((X,y),batchsize=1)\ninput_dim = size(X,1)\nn_hidden = 32\nactivation = relu\noutput_dim = 2\nnn = Chain(\n    Dense(input_dim, n_hidden, activation),\n    Dropout(0.1),\n    Dense(n_hidden, output_dim)\n)\nloss(yhat, y) = Flux.Losses.logitcrossentropy(nn(yhat), y)\n```\n:::\n\n\nNext, we fit the network to the data:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/5\n# Training:\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 20\navg_loss(data) = 0.08936202f0\nEpoch 40\navg_loss(data) = 0.024966659f0\nEpoch 60\navg_loss(data) = 0.01048687f0\nEpoch 80\navg_loss(data) = 0.00838422f0\nEpoch 100\navg_loss(data) = 0.0043536075f0\n```\n:::\n:::\n\n\nTo prepare the fitted model for use with our package, we need to wrap it inside a container. For plain-vanilla models trained in `Flux.jl`, the corresponding constructor is called [`FluxModel`](@ref). There is also a separate constructor called [`FluxEnsemble`](@ref), which applies to Deep Ensembles. Deep Ensembles are a popular approach to approximate Bayesian Deep Learning and have been shown to generate good predictive uncertainty estimates [@lakshminarayanan2016simple].\n\nThe appropriate API call to wrap our simple network in a container follows below:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nM = FluxModel(nn)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nFluxModel(Chain(Dense(2 => 32, relu), Dropout(0.1, active=false), Dense(32 => 2)), :classification_binary)\n```\n:::\n:::\n\n\nThe likelihood function of the output variable is automatically inferred from the data. The generic `plot()` method can be called on the model and data to visualise the results:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nplot(M, counterfactual_data)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](models_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nOur model `M` is now ready for use with the package.\n\n## References\n\n",
    "supporting": [
      "models_files"
    ],
    "filters": []
  }
}