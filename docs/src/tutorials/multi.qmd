---
format: 
  commonmark:
    variant: -raw_html
    wrap: none
    self-contained: true
crossref:
  fig-prefix: Figure
  tbl-prefix: Table
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
output: asis
execute: 
  echo: true
  eval: false
jupyter: julia-1.6
---

```@meta
CurrentModule = CounterfactualExplanations 
```

# Counterfactuals for multi-class targets

In the existing literature counterfactual explanations have typically been applied in the binary classification setting [@verma2020counterfactual]. Research on algorithmic recourse in particular typically involves real-world datasets with an obvious target class - e.g. individual receives credit - and an adverse outcome - e.g. individual is denied loan [@karimi2020survey]. Still, counterfactual explanations are very much also applicable in the multi-class setting. In this tutorial we will go through an illustrative example involving the toy dataset shown in @fig-multi below.

```{julia}
#| echo: false
using Pkg; Pkg.activate("docs")
using Flux, Random, Plots, PlotThemes, CounterfactualExplanations, Statistics
theme(:wong)
include("dev/utils.jl") # some plotting functions
www_path = "docs/src/tutorials/www"
```

```{julia}
using CounterfactualExplanations.Data
using MLJ
p = 3
n = 100
X, ys = make_blobs(n, p, centers=p, as_table=false)
X = X'
using MLUtils
xs = unstack(X,dims=2)
y_train = Flux.onehotbatch(ys, sort(unique(ys)))
y_train = Flux.unstack(y_train',1)
counterfactual_data = CounterfactualData(X,ys')
```

```{julia}
#| echo: false
plt = plot()
plt = scatter!(counterfactual_data)
savefig(plt, joinpath(www_path, "multi_samples.png"))
```

![Synthetic dataset containing four different classes.](www/multi_samples.png){#fig-multi}

## Classifier

To classify the data we use a simple multi-layer perceptron (MLP). In this case the MLP outputs four logits, one for each class. Contrary to the binary setting we therefore choose `logitcrossentropy` as our loss functiona as opposed to `logitbinarycrossentropy`.

```{julia}
n_hidden = 32
out_dim = length(unique(ys))
kw = (output_dim=out_dim,input_dim=size(X,1))
nn = build_model(;kw...)
loss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y)
ps = Flux.params(nn)
data = zip(xs,y_train)
```

The following code just trains the neural network for the task:

```{julia}
using Flux.Optimise: update!, Adam
opt = Adam()
epochs = 100
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10

for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end
```

To make the model compatible with our package we need to 1) declare it as a subtype of `Models.AbstractFittedModel` and 2) dispatch the relevant methods. Logits are returned by the model on call and passed through the softmax function to generate the vector of class probabilities.

```{julia}
M = FluxModel(nn)
```

```{julia}
#| echo: false
plt = plot(M, counterfactual_data)
savefig(plt, joinpath(www_path, "multi_contour.png"))
```


@fig-multi-contour shows the resulting class probabilities in the two-dimensional feature domain.

![Class probabilities for MLP.](www/multi_contour.png){#fig-multi-contour}

# Generating counterfactuals

We randomly select an individual sample from any class and choose any of the remaining classes as our target at random.

```{julia}
# Randomly selected factual:
Random.seed!(42)
x = select_factual(counterfactual_data, rand(1:size(X)[2])) 
y = Flux.onecold(probs(M, x),unique(ys))
target = rand(unique(ys)[1:end .!= y]) # opposite label as target
```

Generic counterfactual search can then be implemented as follows. The only difference to the binary setting is that we need to declare `logitcrossentropy` as the loss function for the counterfactual search. @fig-multi-generic shows the resulting counterfactual path.

```{julia}
# Define generator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, num_counterfactuals=1)
```

```{julia}
#| echo: false
T = size(path(counterfactual))[1]
X_path = reduce(hcat,path(counterfactual))
ŷ = target_probs(counterfactual,X_path)
p1 = plot_contour_multi(X',ys,M;target=target,colorbar=false,title="MLP")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label="p(y′=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:γ],label="threshold γ", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_path, "multi_generic_recourse.gif"), fps=25)
```

![Counterfactual path for generic generator.](www/multi_generic_recourse.gif){#fig-multi-generic}

## Deep ensemble

Staying consistent with previous tutorial we will also briefly look at the Bayesian setting. To incorporate uncertainty we use a simple deep ensemble instead of a single MLP.

```{julia}
ensemble = build_ensemble(5;kw=(output_dim=out_dim,))
ensemble, = forward(ensemble, data, opt, n_epochs=epochs, plot_loss=false)
```

As before, we need to subtype and disptach:

```{julia}
# Step 1)
struct FittedEnsemble <: Models.AbstractFittedModel
    ensemble::AbstractArray
end

# Step 2)
using Statistics
logits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3)
probs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3)

M=FittedEnsemble(ensemble)
```

```{julia}
#| echo: false
plt = plot_contour_multi(X',ys,M);
savefig(plt, joinpath(www_path, "multi_ensemble_contour.png"))
```

@fig-multi-ensemble-contour shows the resulting class probabilities.

![Class probabilities for deep ensemble.](www/multi_ensemble_contour.png){#fig-multi-ensemble-contour}

For the greedy recourse generator we also specify `logitcrossentropy` as our loss function and modify the hyperparameters slightly. @fig-greedy shows the resulting counterfactual path.

```{julia}
generator = GreedyGenerator(loss=:logitcrossentropy,δ=0.25,n=20)
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

```{julia}
#| echo: false
T = size(path(counterfactual))[1]
X_path = reduce(hcat,path(counterfactual))
ŷ = target_probs(counterfactual,X_path)
p1 = plot_contour_multi(X',ys,M;target=target,colorbar=false,title="Deep ensemble")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label="p(y′=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:γ],label="threshold γ", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_path, "multi_greedy_recourse.gif"), fps=25)
```

![Counterfactual path for greedy generator.](www/multi_greedy_recourse.gif){#fig-greedy}

# References