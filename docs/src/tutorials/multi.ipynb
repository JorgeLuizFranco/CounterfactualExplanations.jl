{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```@meta\n",
    "CurrentModule = AlgorithmicRecourse \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recourse for multi-class targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogLevel(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux, Random, Plots, PlotThemes, AlgorithmicRecourse\n",
    "theme(:wong)\n",
    "using Logging\n",
    "disable_logging(Logging.Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = toy_data_multi()\n",
    "X = hcat(x...)\n",
    "y_train = Flux.onehotbatch(y, unique(y))\n",
    "y_train = Flux.unstack(y_train',1)\n",
    "plt = plot()\n",
    "plt = plot_data!(plt,X',y);\n",
    "savefig(plt, \"www/multi_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/multi_samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 32\n",
    "out_dim = length(unique(y))\n",
    "nn = Chain(\n",
    "    Dense(2, n_hidden),\n",
    "    Dense(n_hidden, out_dim)\n",
    ")  \n",
    "loss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y)\n",
    "ps = Flux.params(nn)\n",
    "data = zip(x,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Optimise: update!, ADAM\n",
    "using Statistics\n",
    "opt = ADAM()\n",
    "epochs = 50\n",
    "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
    "\n",
    "using Plots\n",
    "anim = Animation()\n",
    "plt = plot(ylim=(0,avg_loss(data)), xlim=(0,epochs), legend=false, xlab=\"Epoch\")\n",
    "avg_l = []\n",
    "\n",
    "for epoch = 1:epochs\n",
    "  for d in data\n",
    "    gs = gradient(params(nn)) do\n",
    "      l = loss(d...)\n",
    "    end\n",
    "    update!(opt, params(nn), gs)\n",
    "  end\n",
    "  avg_l = vcat(avg_l,avg_loss(data))\n",
    "  plot!(plt, avg_l, color=1, title=\"Average (training) loss\")\n",
    "  frame(anim, plt)\n",
    "end\n",
    "\n",
    "gif(anim, \"www/multi_loss.gif\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/multi_loss.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(Chain(Dense(2, 32), Dense(32, 4)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using AlgorithmicRecourse, AlgorithmicRecourse.Models\n",
    "import AlgorithmicRecourse.Models: logits, probs # import functions in order to extend\n",
    "\n",
    "# Step 1)\n",
    "struct NeuralNetwork <: Models.FittedModel\n",
    "    nn::Any\n",
    "end\n",
    "\n",
    "# Step 2)\n",
    "logits(𝑴::NeuralNetwork, X::AbstractArray) = 𝑴.nn(X)\n",
    "probs(𝑴::NeuralNetwork, X::AbstractArray)= softmax(logits(𝑴, X))\n",
    "𝑴 = NeuralNetwork(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_contour_multi(X',y,𝑴);\n",
    "savefig(plt, \"www/multi_contour.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/multi_contour.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly selected factual:\n",
    "Random.seed!(1234);\n",
    "x̅ = X[:,rand(1:size(X)[2])]\n",
    "y̅ = Flux.onecold(probs(𝑴, x̅),unique(y))\n",
    "target = rand(unique(y)[1:end .!= y̅]) # opposite label as target\n",
    "γ = 0.75\n",
    "# # Define Generator:\n",
    "# generator = GenericGenerator(0.1,0.1,1e-5,:logitcrossentropy,nothing)\n",
    "# # Generate recourse:\n",
    "# recourse = generate_recourse(generator, x̅, 𝑴, target, γ); # generate recourse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
