---
format: 
  commonmark:
    variant: -raw_html
    wrap: none
    self-contained: true
crossref:
  fig-prefix: Figure
  tbl-prefix: Table
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
output: asis
execute: 
  eval: false
  echo: true
jupyter: julia-1.7
---

```@meta
CurrentModule = CounterfactualExplanations 
```

## Variational Autoencoder

### Synthetic data

```{julia}
using CounterfactualExplanations
using Random
Random.seed!(123)
input_dim = 3
n = 100
η = 10
μ₀ = rand(-input_dim:input_dim,input_dim) .* η
μ₁ = rand(-input_dim:input_dim,input_dim) .* η
X₀ = μ₀ .+ randn(input_dim, n) 
X₁ = μ₁ .+ randn(input_dim, n) 
X = hcat(X₀, X₁)
using MLUtils
xs = MLUtils.unstack(X,dims=2)
ys = hcat(zeros(1,n), ones(1,n))
```

```{julia}
using CounterfactualExplanations.GenerativeModels: VAE, train!, reconstruct
vae = VAE(input_dim; epochs=100)
train!(vae, X, ys)
```

```{julia}
using Flux
n_hidden = 50
nn = Chain(
    Dense(input_dim, n_hidden, relu),
    Dense(n_hidden, 1)
)  
loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y)
data = zip(xs,ys)
```

```{julia}
using Flux.Optimise: update!, ADAM
opt = ADAM()
epochs = 100
using Statistics: mean
avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))
show_every = epochs/10

for epoch = 1:epochs
  for d in data
    gs = gradient(Flux.params(nn)) do
      l = loss(d...)
    end
    update!(opt, Flux.params(nn), gs)
  end
  if epoch % show_every == 0
    println("Epoch " * string(epoch))
    @show avg_loss(data)
  end
end

M = FluxModel(nn)
```

```{julia}
counterfactual_data = CounterfactualData(X,ys)
```

```{julia}
using Random
Random.seed!(123)
x = select_factual(counterfactual_data, rand(1:size(X)[2])) 
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
```

```{julia}
# Define generator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=true, γ=0.9)
```

```{julia}
#| echo: false
include("docs/src/utils.jl")
plt = plot(counterfactual.data.generative_model, X, ys)
T = size(path(counterfactual))[1]
X_path = reduce(hcat,path(counterfactual))
scatter!([X_path[1,1]],[X_path[2,1]],colour=:yellow,ms=5,label="")
scatter!([X_path[1,2:end]],[X_path[2,2:end]],colour=:yellow,ms=5,label="")
```


### MNIST data

```{julia}
#| echo: false
using CounterfactualExplanations, Plots, MLDatasets
using MLDatasets: convert2image
using BSON: @save, @load
theme(:wong)
include("dev/utils.jl") # some plotting functions
www_path = "docs/src/generators/www"
```

```{julia}
#| echo: false
data_train = MNIST(:train)
train_x, train_y = data_train[:]
input_dim = prod(size(train_x[:,:,1]))
using Images, Random, StatsBase
Random.seed!(1)
n_samples = 10
samples = train_x[:,:,sample(1:end, n_samples, replace=false)]
mosaic = mosaicview([convert2image(MNIST, samples[:,:,i]) for i ∈ 1:n_samples]...,ncol=Int(n_samples/2))
plt = plot(mosaic, size=(500,260), axis=nothing, background=:transparent)
savefig(plt, joinpath(www_path, "mnist_samples.png"))
```

```{julia}
using Flux
using CounterfactualExplanations.Data: mnist_data
X, ys = mnist_data()
```

```{julia}
using CounterfactualExplanations.GenerativeModels: VAE, train!
vae = VAE(input_dim)
train!(vae, X, ys)
```

```{julia}
using CounterfactualExplanations.GenerativeModels
plt = GenerativeModels.plot(vae, X, ys)
savefig(plt, joinpath(www_path, "vae_output.png"))
```
