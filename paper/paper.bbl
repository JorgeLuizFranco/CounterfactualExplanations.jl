\begin{thebibliography}{10}

\bibitem{arrieta2020explainable}
Alejandro~Barredo Arrieta, Natalia Diaz-Rodriguez, Javier Del~Ser, Adrien
  Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez,
  Daniel Molina, Richard Benjamins, et~al.
Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities
  and challenges toward responsible ai.
{\em Information Fusion}, 58:82--115, 2020.

\bibitem{borch2022machine}
Christian Borch.
Machine learning, knowledge risk, and principal-agent problems in automated
  trading.
{\em Technology in Society}, page 101852, 2022.

\bibitem{buolamwini2018gender}
Joy Buolamwini and Timnit Gebru.
Gender shades: Intersectional accuracy disparities in commercial gender
  classification.
In {\em Conference on fairness, accountability and transparency}, pages 77--91.
  PMLR, 2018.

\bibitem{daxberger2021laplace}
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
  Matthias Bauer, and Philipp Hennig.
Laplace redux-effortless bayesian deep learning.
{\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{fan2020interpretability}
Fenglei Fan, Jinjun Xiong, and Ge~Wang.
On interpretability of artificial neural networks.
{\em Preprint at https://arxiv. org/abs/2001.02522}, 2020.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
Dropout as a bayesian approximation: Representing model uncertainty in deep
  learning.
In {\em international conference on machine learning}, pages 1050--1059. PMLR,
  2016.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples.
{\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{hansen2020virtue}
Kristian~Bondo Hansen.
The virtue of simplicity: On machine learning models in algorithmic trading.
{\em Big Data \& Society}, 7(1):2053951720926558, 2020.

\bibitem{innes2018flux}
Mike Innes.
Flux: Elegant machine learning with julia.
{\em Journal of Open Source Software}, 3(25):602, 2018.

\bibitem{joshi2019towards}
Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep
  Ghosh.
Towards realistic individual recourse and actionable explanations in black-box
  decision making systems.
{\em arXiv preprint arXiv:1907.09615}, 2019.

\bibitem{karimi2020survey}
Amir-Hossein Karimi, Gilles Barthe, Bernhard Sch{\"o}lkopf, and Isabel Valera.
A survey of algorithmic recourse: definitions, formulations, solutions, and
  prospects.
{\em arXiv preprint arXiv:2010.04050}, 2020.

\bibitem{karimi2021algorithmic}
Amir-Hossein Karimi, Bernhard Sch{\"o}lkopf, and Isabel Valera.
Algorithmic recourse: from counterfactual explanations to interventions.
In {\em Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
  Transparency}, pages 353--362, 2021.

\bibitem{karimi2020algorithmic}
Amir-Hossein Karimi, Julius Von~K{\"u}gelgen, Bernhard Sch{\"o}lkopf, and
  Isabel Valera.
Algorithmic recourse under imperfect causal knowledge: a probabilistic
  approach.
{\em arXiv preprint arXiv:2006.06831}, 2020.

\bibitem{kaur2020interpreting}
Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and
  Jennifer Wortman~Vaughan.
Interpreting interpretability: understanding data scientists' use of
  interpretability tools for machine learning.
In {\em Proceedings of the 2020 CHI conference on human factors in computing
  systems}, pages 1--14, 2020.

\bibitem{lakshminarayanan2016simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
Simple and scalable predictive uncertainty estimation using deep ensembles.
{\em arXiv preprint arXiv:1612.01474}, 2016.

\bibitem{lecun1998mnist}
Yann LeCun.
The mnist database of handwritten digits.
{\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{miller2019explanation}
Tim Miller.
Explanation in artificial intelligence: Insights from the social sciences.
{\em Artificial intelligence}, 267:1--38, 2019.

\bibitem{molnar2020interpretable}
Christoph Molnar.
{\em Interpretable machine learning}.
Lulu. com, 2020.

\bibitem{mothilal2020explaining}
Ramaravind~K Mothilal, Amit Sharma, and Chenhao Tan.
Explaining machine learning classifiers through diverse counterfactual
  explanations.
In {\em Proceedings of the 2020 Conference on Fairness, Accountability, and
  Transparency}, pages 607--617, 2020.

\bibitem{murphy2022probabilistic}
Kevin~P Murphy.
{\em Probabilistic Machine Learning: An introduction}.
MIT Press, 2022.

\bibitem{oecd2021artificial}
OECD.
Artificial intelligence, machine learning and big data in finance:
  Opportunities, challenges and implications for policy makers, 2021.

\bibitem{pawelczyk2021carla}
Martin Pawelczyk, Sascha Bielawski, Johannes van~den Heuvel, Tobias Richter,
  and Gjergji Kasneci.
Carla: a python library to benchmark algorithmic recourse and counterfactual
  explanation algorithms.
{\em arXiv preprint arXiv:2108.00783}, 2021.

\bibitem{poyiadzi2020face}
Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De~Bie, and Peter
  Flach.
Face: Feasible and actionable counterfactual explanations.
In {\em Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages 344--350, 2020.

\bibitem{rudin2019stop}
Cynthia Rudin.
Stop explaining black box machine learning models for high stakes decisions and
  use interpretable models instead.
{\em Nature Machine Intelligence}, 1(5):206--215, 2019.

\bibitem{schut2021generating}
Lisa Schut, Oscar Key, Rory Mc~Grath, Luca Costabello, Bogdan Sacaleanu, Yarin
  Gal, et~al.
Generating interpretable counterfactual explanations by implicit minimisation
  of epistemic and aleatoric uncertainties.
In {\em International Conference on Artificial Intelligence and Statistics},
  pages 1756--1764. PMLR, 2021.

\bibitem{slack2021counterfactual}
Dylan Slack, Anna Hilgard, Himabindu Lakkaraju, and Sameer Singh.
Counterfactual explanations can be manipulated.
{\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{slack2020fooling}
Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
Fooling lime and shap: Adversarial attacks on post hoc explanation methods.
In {\em Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages 180--186, 2020.

\bibitem{sturm2014simple}
Bob~L Sturm.
A simple method to determine if a music information retrieval system is a
  ``horse''.
{\em IEEE Transactions on Multimedia}, 16(6):1636--1644, 2014.

\bibitem{upadhyay2021towards}
Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju.
Towards robust and reliable algorithmic recourse.
{\em arXiv preprint arXiv:2102.13620}, 2021.

\bibitem{ustun2019actionable}
Berk Ustun, Alexander Spangher, and Yang Liu.
Actionable recourse in linear classification.
In {\em Proceedings of the Conference on Fairness, Accountability, and
  Transparency}, pages 10--19, 2019.

\bibitem{varshney2022trustworthy}
Kush~R. Varshney.
{\em Trustworthy Machine Learning}.
Independently Published, Chappaqua, NY, USA, 2022.

\bibitem{verma2020counterfactual}
Sahil Verma, John Dickerson, and Keegan Hines.
Counterfactual explanations for machine learning: A review.
{\em arXiv preprint arXiv:2010.10596}, 2020.

\bibitem{wachter2017counterfactual}
Sandra Wachter, Brent Mittelstadt, and Chris Russell.
Counterfactual explanations without opening the black box: Automated decisions
  and the gdpr.
{\em Harv. JL \& Tech.}, 31:841, 2017.

\end{thebibliography}
