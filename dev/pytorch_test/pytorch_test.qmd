# Notebook for testing the pytorch model
```{julia}
using Revise
```
### Set up
```{julia}
# Don't use CONDA
ENV["JULIA_CONDAPKG_BACKEND"] = "Null"

# Dependencise
using Pkg
Pkg.activate("$(pwd())/dev/pytorch_test")

using PythonCall
using CounterfactualExplanations
```

# Load data
```{julia}
using Random
# Some random data:
Random.seed!(1234);
N = 100

@info "Generating data..."
counterfactual_data = CounterfactualExplanations.Data.load_blobs(N)

x = counterfactual_data.X
y = counterfactual_data.y

x = PyArray(x)
y = PyArray(y)

@info "Data generated."
```

# Separate x and y data
```{julia}
torch = pyimport("torch")
np = pyimport("numpy")

x = counterfactual_data.X
x = np.array(x)
x = torch.Tensor(x).T

y = counterfactual_data.y
y = np.array(y[1, :])
y = torch.Tensor(y)
```

# Define model
```{julia}
MLP = PythonCall.pytype("MLP", (torch.nn.Module,), [
    "__module__" => "__main__",

    pyfunc(
        name = "__init__",
        function(self)
            # Not fully sure about the following line
            torch.nn.Module.__init__(self)
            self.model = torch.nn.Sequential(
                torch.nn.Flatten(),
                torch.nn.Linear(2, 32),
                torch.nn.Sigmoid(),
                torch.nn.Linear(32, 1)
            )
            return
        end
    ),

    pyfunc(
        name = "forward",
        function(self, x)
            return self.model(x)
        end
    )
])
```

# Declare model, optimizer and loss function
```{julia}
@info "Training"
model = MLP()
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
loss_fun = torch.nn.BCEWithLogitsLoss()
```

# Training
```{julia}
for epoch in 1:100
  # Compute prediction and loss:
  @info "Epoch $epoch"
  output = model(x).squeeze()

  @info "Loss"
  println(y.shape)
  loss = loss_fun(output, y)
  
  @info "Backpropagation"
  # Backpropagation:
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  println("Loss at epoch $epoch: $(loss.item())")
end
```

# Declare MyPyTorchModel
```{julia}
using Flux
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend

struct MyPyTorchModel <: AbstractDifferentiableModel
    nn::Any
    likelihood::Symbol
end

function logits(M::MyPyTorchModel, X::AbstractArray)    
    if !isa(X, Matrix)
      X = reshape(X, length(X), 1)
    end

    ŷ_python = M.nn(torch.Tensor(np.array(X)).T).detach().numpy()
    ŷ = []
    for i in ŷ_python
        
    end
    ŷ = pyconvert(Vector, ŷ)
    println("=====")
    println(typeof(ŷ))

    # There was a transpose operation but we don't understand it and we believe it's not necessary
    return ŷ'
end

function probs(M::MyPyTorchModel, X::AbstractArray)
    println("It's idiotic")
    return σ.(logits(M, X))
end

M = MyPyTorchModel(model, :classification_binary)

# ----- PyTorch Model ----- #
# using PyCall
# struct PyTorchModel <: AbstractDifferentiableModel
#     nn::Any
# end

# function logits(M::PyTorchModel, X::AbstractArray)
#     py"""
#     import torch
#     from torch import nn
#     """
#     nn = M.nn
#     if !isa(X, Matrix)
#       X = reshape(X, length(X), 1)
#     end
#     ŷ = py"$nn(torch.Tensor($X).T).detach().numpy()"
#     ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]
#     return ŷ'
# end

# probs(M::PyTorchModel, X::AbstractArray)= σ.(logits(M, X))
```

# Declare ∂ℓ
```{julia}
import CounterfactualExplanations.Generators: ∂ℓ
using LinearAlgebra

# Countefactual loss:
function ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyPyTorchModel, counterfactual_state::CounterfactualExplanation) 
  nn = M.nn
  x′ = counterfactual_state.x
  t = counterfactual_state.target
  x = reshape(x′, 1, length(x′))

  x = np.array(x)
  x = torch.Tensor(x)
  x.requires_grad = true

  t = np.array(t)
  t = torch.Tensor(t).squeeze()

  output = nn(x).squeeze()

  obj_loss = nn.BCEWithLogitsLoss()(output,t)
  obj_loss.backward()

  # grad = vec(py"x.grad.detach().numpy()")

  grad = vec(x.grad.detach().numpy())

  return grad
end
```

```{julia}
Random.seed!(123)

x_random_factual = select_factual(counterfactual_data, rand(1:length(x))) 
y_tmp = probs(M, x_random_factual)
y_random_factual = round(y_tmp[1])

target = ifelse(y_random_factual==1.0,0.0,1.0)

# Random.seed!(123)
# x = select_factual(counterfactual_data, rand(1:length(xs))) 
# y = round(probs(M, x)[1])
# target = ifelse(y==1.0,0.0,1.0) # opposite label as target
```

```{julia}
# Define a generator:
generator = GenericGenerator()

# Generate a recourse:

println(size(counterfactual_data.X))
println(x_random_factual)

counterfactual = generate_counterfactual(
    x_random_factual, 
    target, 
    counterfactual_data, 
    M, 
    generator
)
```

```{julia}
#| echo: false
T = total_steps(counterfactual)
X_path = reduce(hcat,path(counterfactual))
ŷ = target_probs(counterfactual,X_path)
p1 = plot_contour(X',ys,M;colorbar=false, title="Posterior predictive")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label="p(y′=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:γ],label="threshold γ", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_path, "interop_py.gif"), fps=5)
```

```{julia}
j = [1 2]
j = np.array(j)

println(j.shape)
e = []

for i in j
    tmp = pyconvert(Vector, i)
    println(typeof(tmp))
    # push!(e, pyconvert(Float32, i))
end

println(typeof(j))
```
