---
title: Explaining Black-Box Models through Counterfactuals
subtitle: JuliaCon 2022
author: Patrick Altmeyer
format: 
  revealjs:
    logo: www/delft_logo.png
    footer: JuliaCon 2022 - Explaining Black-Box Models through Counterfactuals
    self-contained: true
    smaller: true
    scrollable: true
    preview-links: auto
    fig-align: center
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
execute:
  eval: false
  echo: true
---

```{julia}
using Pkg; Pkg.activate("dev")
using PlotThemes
theme(:wong)
include("dev/utils.jl")
www_path = "dev/resources/juliacon22/www"
```


## Overview

:::{.incremental}
- The Problem with Black Boxes ⬛
    - What are black-box models? Why do we need explainability?
- Enter: Counterfactual Explanations 🔮
    - What are counterfactuals? What are they not?
- [`CounterfactualExplanations.jl`](https://www.paltmeyer.com/CounterfactualExplanations.jl/stable/) in Julia (and beyond!) 📦
    - What can it do?
    - Usage examples
    - Design choices
- Goals and Ambitions 🎯
    - Where can it go?
    - How to contribute
:::

# The Problem with Black Boxes ⬛

## Of Short Lists, Pandas and Gibbons

- From human to data-driven decision-making:
  - Black-box models like deep neural networks are being deployed virtually everywhere.
  - More likely than not that your loan or employment application is handled by an algorithm.
  - Includes critical domains: health care, autonomous driving, finance, ... 

. . .

> A recipe for disaster ...

. . .

- We have no idea what exactly we're cooking up ...
    - Have you received an automated rejection email? Why didn't you "mEet tHe sHoRtLisTiNg cRiTeRia"? 🙃

. . .

- ... but we do know that some of it is junk. 

![Adversarial attacks on deep neural networks. Source: @goodfellow2014explaining](www/panda.png){#fig-panda width=60%}

## "Weapons of Math Destruction"

- If left unchallenged, these properties of black-box models can create undesirable dynamics: 
  - Human operators in charge of the system have to rely on it blindly.
  - Those individuals subject to it generally have no way to challenge an outcome.

> “You cannot appeal to (algorithms). They do not listen. Nor do they bend.”
>
> — Cathy O'Neil in [*Weapons of Math Destruction*](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction), 2016

![Cathy O’Neil. Source: Cathy O’Neil](www/cathy.webp){#fig-cathy width=60%}

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}

::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Data
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Counterfactual Reasoning
:::
:::

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack style="text-align: center;"}
::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Data
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Counterfactual Reasoning
:::
:::

**Model objective**: maximize $p(\mathcal{D}|\theta)$ where $\mathcal{D}=\{(x,y)\}_{i=1}^n$ (supervised)

:::{.incremental}
- In an ideal world we can rely on parsimonious and interpretable models.
- In practice these models have their limitation.
- Black-box models like deep neural network are the very opposite of parsimonious.
:::

. . .

> [...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. 
> [@wilson2020case]

- In this setting it is often crucial to treat models probabilistically!

. . .

- Probabilistic models covered briefly today. More in my other talk ...

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}
::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Data
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; align-items: center;"}
Counterfactual Reasoning
:::
:::

:::{.incremental}
- Counterfactual reasoning boils down to simple questions: what if $x \Rightarrow x\prime$?
- By (strategically) perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.
:::

. . .

> Even though [...] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the “black box”.
> [@wachter2017counterfactual]

# Enter: Counterfactual Explanations 🔮

## A Framework for Counterfactual Explanations
 
- Objective originally proposed by @wachter2017counterfactual is as follows where $h$ relates to the complexity of the counterfactual and $M$ denotes the classifier:

$$
\min_{x\prime \in \mathcal{X}} h(x\prime) \ \ \ \mbox{s. t.} \ \ \ M(x\prime) = t
$$ {#eq-obj}

- Typically approximated through regularization:

$$
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)
$$ {#eq-solution} 

## Toy example

#### A simple counterfactual path from 🐱 to 🐶 

We have fitted some black-box classifier to divide cats and dogs. One 🐱 is friends with a lot of cool 🐶  and wants to remain part of that group. The counterfactual path below shows her how to fool the classifier:

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_mlp.gif){#fig-cat-mlp}

## Counterfactuals ... as in Adversarial Examples?

> Yes and no! 

While both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.

- Effective counterfactuals should meet certain criteria ✅

- **closeness**: the average distance between factual and counterfactual features should be small (@wachter2017counterfactual)
- **actionability**: the proposed feature perturbation should actually be actionable (@ustun2019actionable, @poyiadzi2020face)
- **plausibility**: the counterfactual explanation should be plausible to a human (@joshi2019towards)
- **unambiguity**: a human should have no trouble assigning a label to the counterfactual (@schut2021generating)
- **sparsity**: the counterfactual explanation should involve as few individual feature changes as possible (@schut2021generating)
- **robustness**: the counterfactual explanation should be robust to domain and model shifts (@upadhyay2021towards)
- **diversity**: ideally multiple diverse counterfactual explanations should be provided (@mothilal2020explaining)
- **causality**: counterfactual explanations reflect the structural causal model underlying the data generating process (@karimi2020algorithmic, @karimi2021algorithmic)

## Counterfactuals ... as in Causal Inference?

> NO!

:::{.incremental}
- In causal inference (Potential Outcome Framework), counterfactuals are unobserved states of the world that we would like to observe in order to establish causality.
  - For example, in drug trials we would love to observe the **actual outcome** under treatment ($y_i|T=1$) and non-treatment ($y_i|T=0$) for the **same** individual.
  - Instead we estimate an average treatment effect as: 
- CE involves perturbing features **after** some model has been trained.
  - We end up comparing **modeled outcomes** $f(x_i)$ and $f(x_i\prime)$.
- The two are **NOT** the same!
:::

. . .

> Well, maybe ...

There is nonetheless an intriguing link between the two domains:

:::{.incremental}
- If we do have causal knowledge, let's leverage it: from minimal **perturbations** to minimal **interventions** (@karimi2020algorithmic, @karimi2021algorithmic)
- If CEs that rely on minimal interventions fail, does that not provide some evidence that the assumed causal graph is inaccurate?
- Very much a open research field ...
:::


## Probabilistic Methods for Counterfactual Explanations

When people say that counterfactuals should look **realistic** or **plausible**, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:

$$
x\prime \sim p(x)
$$

- How do we estimate $p(x)$? Two probabilistic approaches ...

::: {.panel-tabset}

### APPROACH 1: use the model itself

- @schut2021generating note that by maximizing predictive probabilities $\sigma(M(x\prime))$ for **probabilistic** models $M\in\mathcal{\widetilde{M}}$ one implicitly minimizes **epistemic** and **aleotoric** uncertainty.

### APPROACH 2: use some generative model

- Instead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model.

![Counterfactual (yellow) generated through latent space search (right panel) following @joshi2019towards. The corresponding counterfactual path in the feature space is shown in the left panel.](www/example_3d.png){#fig-latent-3d}

:::

# Counterfactual Explanations in Julia (and beyond!)

## Limited Software Availability  

- Some of the existing approaches scattered across different GitHub repositories (🐍).
- Only one unifying Python 🐍 library: CARLA [@pawelczyk2021carla].
    - Comprehensive and (somewhat) extensible ...
    - ... but not language-agnostic and some desirable functionality not supported.
    - Also not composable: each generator is treated as different class/entity.
- Both R and Julia lacking any kind of implementation. Until now ...

## Enter: `CounterfactualExplanations.jl` 📦

[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/dev)
[![Build Status](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl)

:::{.incremental}
- A unifying framework for generating Counterfactual Explanations.
- Built in Julia, but essentially language agnostic:
    - Currently supporting explanations for differentiable models built in Julia (e.g. Flux) and torch (R and Python).
- Designed to be easily extensible through dispatch.
- Designed to be composable allowing users and developers to combine different counterfactual generators.
:::

. . .

> Julia has an edge with respect to Trustworthy AI: it's open-source, uniquely transparent and interoperable 🔴🟢🟣

# Package Architecture

> Modular, composable, scalable! 

## Overview

![Overview of package architecture. Modules are shown in red, structs in green and functions in blue.](../pkg_architecture.png){#fig-architecture width="70%"}

## Generators 

```{julia}
using CounterfactualExplanations, Plots, GraphRecipes
plt = plot(AbstractGenerator, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))
savefig(plt, joinpath(www_path,"generators.png"))
```

![Type tree for `AbstractGenerator`.](www/generators.png){#fig-generators width="60%"}

## Models

```{julia}
plt = plot(AbstractFittedModel, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))
savefig(plt, joinpath(www_path,"models.png"))
```

![Type tree for `AbstractFittedModel`.](www/models.png){#fig-models width="60%"}

# Basic Usage

## A simple example

1. Load and prepare some toy data.
2. Select a random sample.
3. Generate counterfactuals using different approaches.

```{julia}
# Data:
using CounterfactualExplanations.Data
N = 100
xs, ys = Data.toy_data_linear(N)
X = hcat(xs...)
counterfactual_data = CounterfactualData(X,ys')

# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(X)[2]))
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
```


## Generic Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
# Model
using CounterfactualExplanations.Models: LogisticModel
w = [1.0 1.0] # estimated coefficients
b = 0 # estimated bias
M = LogisticModel(w, [b])

# Counterfactual search:
generator = GenericGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
function anim_path(counterfactual)
    T = total_steps(counterfactual)
    X_path = reduce(hcat,path(counterfactual))
    ŷ = target_probs(counterfactual,X_path)
    p1 = plot_contour(X',ys,M;colorbar=false, title="Counterfactual Path")
    anim = @animate for t in 1:T
        scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
        p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label="p(y′=" * string(target) * ")", title="Validity", lc=:black)
        Plots.abline!(p2,0,counterfactual.params[:γ],label="threshold γ", ls=:dash) # decision boundary
        plot(p1,p2,size=(800,400))
    end
end
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "binary_generic_recourse.gif"), fps=25)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `GenericGenerator`.](www/binary_generic_recourse.gif){#fig-generic}
:::

::::

## Greedy Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
using LinearAlgebra
Σ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix
μ = hcat(b, w)
M = CounterfactualExplanations.Models.BayesianLogisticModel(μ, Σ)

# Counterfactual search:
generator = GreedyGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_greedy.gif"), fps=25)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `GreedyGenerator`.](www/example_greedy.gif){#fig-greedy}
:::

::::

## REVISE Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
# Counterfactual search:
generator = REVISEGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
# Counterfactual search:
generator = GenericGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator,
  latent_space=true
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_revise.gif"), fps=2)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `REVISEGenerator`.](www/example_revise.gif){#fig-greedy}
:::

::::

## A more involved example

![Counterfactual explanations for MNIST data. Turning a nine (9) into a four (4).](www/MNIST_9to4.png){#fig-mnist-schut}

![Turning a nine (9) into a four (4) using generic search in the feature space. It appears that the VAE is well-specified in this case.](www/mnist_9to4_latent.png){#fig-mnist-latent}

But things can go wrong ...

![](www/mnist_7to9_latent.png)

![](www/mnist_7to9_wachter.png)

# Customization

## Custom Models

:::{.panel-tabset}

### Counterfactual path

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/interop_r.gif)

### R

#### Subtyping and dispatch

```{.julia}
using Flux, RCall
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend

# Step 1)
struct TorchNetwork <: Models.AbstractFittedModel
    model::Any
end

# Step 2)
function logits(M::TorchNetwork, X::AbstractArray)
  nn = M.model
  ŷ = rcopy(R"as_array($nn(torch_tensor(t($X))))")
  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]
  return ŷ'
end
probs(M::TorchNetwork, X::AbstractArray)= σ.(logits(M, X))
M = TorchNetwork(R"model")
```

#### Gradient access

```{.julia}
import CounterfactualExplanations.Generators: ∂ℓ
using LinearAlgebra

# Countefactual loss:
function ∂ℓ(generator::AbstractGradientBasedGenerator, counterfactual_state::CounterfactualState) 
  M = counterfactual_state.M
  nn = M.model
  x′ = counterfactual_state.x′
  t = counterfactual_state.target_encoded
  R"""
  x <- torch_tensor($x′, requires_grad=TRUE)
  output <- $nn(x)
  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)
  obj_loss$backward()
  """
  grad = rcopy(R"as_array(x$grad)")
  return grad
end
```

:::

## Custom Generators

::: {.panel-tabset}

### Generic generator with dropout

![](www/dropout_recourse.gif)

### Code

#### Subtyping

```{.julia}
# Abstract suptype:
abstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end

# Constructor:
struct DropoutGenerator <: AbstractDropoutGenerator
    loss::Symbol # loss function
    complexity::Function # complexity function
    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints 
    λ::AbstractFloat # strength of penalty
    ϵ::AbstractFloat # step size
    τ::AbstractFloat # tolerance for convergence
    p_dropout::AbstractFloat # dropout rate
end

# Instantiate:
using LinearAlgebra
generator = DropoutGenerator(
    :logitbinarycrossentropy,
    norm,
    nothing,
    0.1,
    0.1,
    1e-5,
    0.5
)
```

#### Dispatch

```{.julia}
import CounterfactualExplanations.Generators: generate_perturbations, ∇
using StatsBase
function generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::CounterfactualState)
    𝐠ₜ = ∇(generator, counterfactual_state) # gradient
    # Dropout:
    set_to_zero = sample(1:length(𝐠ₜ),Int(round(generator.p_dropout*length(𝐠ₜ))),replace=false)
    𝐠ₜ[set_to_zero] .= 0
    Δx′ = - (generator.ϵ .* 𝐠ₜ) # gradient step
    return Δx′
end
```

:::

# Goals and Ambitions 🎯

## JuliaCon 2022 and beyond

- To be submitted to [JuliaCon 2022](https://juliacon.org/2022/) (today 🫣)
- Through the help of community contribution we hope to add:
    - native support for deep learning models (`Flux`, `torch`, `tensorflow`) and other differentiable models.
    - support for non-differentiable models.
    - more generators (DiCE [@mothilal2020explaining], ROAR [@upadhyay2021towards], MINT [@karimi2021algorithmic], CLUE [@antoran2020getting])

## More Resources

- Introductory [blog post: [[TDS](https://towardsdatascience.com/individual-recourse-for-black-box-models-5e9ed1e4b4cc), [homepage](https://www.paltmeyer.com/blog/posts/individual-recourse-for-black-box-models/)]
- Package [docs](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/) with lots of examples
- Blog post on effortless Bayesian deep learning [[TDS]((https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b)), [homepage](https://www.paltmeyer.com/blog/posts/effortsless-bayesian-dl/)]
- [Get in touch](https://www.paltmeyer.com/)!

# Hidden

## Explainable AI (XAI)

- *interpretable* = inherently interpretable model, no extra tools needed (GLM, decision trees, rules, ...)  [@rudin2019stop]
- *explainable* = inherently not interpretable model, but explainable through XAI

#### Post-hoc Explainability:
- Local **surrogate explainers** like LIME and Shapley: useful and popular, but ... 
    - ... can be easily fooled [@slack2020fooling]
    - ... rely on reasonably interpretable features.
    - ... rely on the concept of fidelity.
- **Counterfactual explanations** explain how inputs into a system need to change for it to produce different decisions. 
    - Always full-fidelity, since no proxy involved. 
    - Intuitive interpretation and straight-forward implemenation.
    - Works well with Bayesian models. Clear link to Causal Inference. 
    - Does not rely on interpretable features.
- Realistic and actionable changes can be used for the purpose of **algorithmic recourse**.

## Feature Constraints

::: {.panel-tabset}

### Domain constraint

![](www/mutability_domain_2.gif)

### Code

Mutability constraints can be added at the preprocessing stage:

```{.julia}
counterfactual_data = CounterfactualData(X,ys';domain=[(-Inf,Inf),(-Inf,-0.5)])
```

:::

## Research Topics (1) - Student Project

> What happens once AR has actually been implemented? 👀

:::{.incremental}
- Towards robust AR: protection against exogenous domain and model shifts [@upadhyay2021towards]
- What about endogenous model shifts?
:::

![](www/bayesian.gif){fig-align="center" width=800px} 

## Research Topics (2)

:::{.incremental}
- An effortless way to incorporate model uncertainty (w/o need for expensive generative model): *Laplace Redux*.
- Counterfactual explanations for time series data.
- Is CE really more intuitive? Could run a user-based study like in @kaur2020interpreting.
- More ideas form your side? 🤗
:::

## References 